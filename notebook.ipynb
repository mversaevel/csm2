{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147b6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Initialize, import libraries and load data, clean data \"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# load market data\n",
    "files_to_load = [\n",
    "    'BAA10Y', # yield spread between BAA-rated corporate bonds and 10-year Treasury bonds\n",
    "    'T10Y3M', # term spread between 10-year Treasury bonds and 3-month Treasury bills\n",
    "    'DTB4WK', # 4-week Treasury bill rate - used as a proxy for the risk-free rate\n",
    "    'DTB3' # 3-month Treasury bill rate\n",
    "]\n",
    "collected_market_data = []\n",
    "for filename in files_to_load:\n",
    "    data = pd.read_csv(f'data/{filename}.csv')\n",
    "    data['observation_date'] = pd.to_datetime(data['observation_date']) - pd.offsets.Day(1)\n",
    "    data = data.rename(columns={\n",
    "        'observation_date': 'Date',\n",
    "    }).set_index('Date')\n",
    "    collected_market_data.append(data)\n",
    "\n",
    "df_market_data = pd.concat(collected_market_data, axis=1)\n",
    "df_market_data['BAA10Y'] /= 100\n",
    "df_market_data['T10Y3M'] /= 100\n",
    "df_market_data['DTB3'] /= 100\n",
    "df_market_data['DTB4WK'] = np.log(1 + df_market_data['DTB4WK'] / 100) # make percentage, take log, add 1 to avoid log(0)\n",
    "\n",
    "\n",
    "CAPE = pd.read_excel('data/ie_data.xls', sheet_name='Data', skiprows=7, header=0)[['Date', 'CAPE']]\n",
    "CAPE['Date'] = pd.to_datetime(CAPE['Date'].astype(str), format='%Y.%m')\n",
    "CAPE = CAPE.dropna()\n",
    "CAPE['Date'] = CAPE['Date'] - pd.offsets.Day(1)  # Adjusting the date to the next day\n",
    "CAPE['CAPE'] = np.log(CAPE['CAPE'])  # Taking the log of CAPE for normalization\n",
    "CAPE = CAPE.set_index('Date')\n",
    "\n",
    "df_market_data = df_market_data.join(CAPE, how='left')\n",
    "df_market_data = df_market_data[~df_market_data.index.duplicated(keep='first')]\n",
    "\n",
    "# load CSM index data\n",
    "data_path = 'data/index_data/'\n",
    "\n",
    "with open(data_path + 'run4-mimicking.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "trad_country_returns = results[\"trad_country_returns\"]\n",
    "csm_returns = results[\"csm_returns\"]\n",
    "\n",
    "trad_country_returns_excess = []\n",
    "for country in trad_country_returns.columns:\n",
    "    working_df = trad_country_returns[country].copy()\n",
    "    working_df = working_df.reset_index()\n",
    "    working_df['Date'] = pd.to_datetime(working_df['Date'])\n",
    "    working_df[country] = np.log(1 + working_df[country])\n",
    "    working_df = working_df.set_index('Date')\n",
    "    working_df = working_df.join(df_market_data['DTB4WK'])\n",
    "    country_excess = working_df[country] - working_df['DTB4WK']\n",
    "    trad_country_returns_excess.append(country_excess)\n",
    "\n",
    "trad_country_returns_excess = pd.concat(trad_country_returns_excess, axis=1)\n",
    "trad_country_returns_excess.columns = trad_country_returns.columns\n",
    "trad_country_returns_excess\n",
    "\n",
    "csm_returns_excess = []\n",
    "for country in csm_returns.columns:\n",
    "    working_df = csm_returns[country].copy()\n",
    "    working_df = working_df.reset_index()\n",
    "    working_df['Date'] = pd.to_datetime(working_df['Date'])\n",
    "    working_df[country] = np.log(1 + working_df[country])\n",
    "    working_df = working_df.set_index('Date')\n",
    "    working_df = working_df.join(df_market_data['DTB4WK'])\n",
    "    country_excess = working_df[country] - working_df['DTB4WK']\n",
    "    csm_returns_excess.append(country_excess)\n",
    "\n",
    "csm_returns_excess = pd.concat(csm_returns_excess, axis=1)\n",
    "csm_returns_excess.columns = csm_returns.columns\n",
    "csm_returns_excess\n",
    "\n",
    "### getting global returns is still work in progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6c9a0",
   "metadata": {},
   "source": [
    "This section provides mean-variance spanning tests (Huberman and Kandel, 1988). This methodology has previously been applied in the context of international diversification. Mean-variance spanning works by selecting a set of initial asset, commonly called benchmark assets. Next to that, there are test assets. Mean-variance spanning provides an answer to the question of whether the mean-variance efficient frontier of the test assets is the same as that of the benchmark assets.\n",
    "\n",
    "Huberman and Kandel's (HK) approach is that of a regression:\n",
    "\n",
    "Y = a + bX + e\n",
    "\n",
    "Where Y is an N x t matrix of returns of test assets; X is a K x t matrix of returns of benchmark assets; a is 1 x K vector of intercepts coefficients and b a 1 x K vector of coefficients. [look at dimensions; do not yet match!]\n",
    "\n",
    "The HK test looks at the intercepts - in the case where the two mean-variance frontiers are the same, the intercepts will also be 0. Or put differently, when the intercepts are not statistically significant from zero, we can infer that the mean-variance frontier of the test assets is not significantly different from that of the benchmark assets. This, in turn, implies that there are no diversification benefits from adding the test assets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3b1016",
   "metadata": {},
   "source": [
    "First we perform iterative mean spanning tests, where have as N benchmark assets the traditional country indices; and each time, we add a different test asset K to the mix. In these cases, K is always 1. The set of K test assets to draw from is the set of individual CSM country indices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faaf694",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8d0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spanning_results = {}\n",
    "for country in csm_returns.columns:\n",
    "    X = sm.add_constant(csm_returns[country])\n",
    "    y = csm_returns[country]\n",
    "    model = sm.OLS(y, X).fit()\n",
    "    spanning_results[country] = model\n",
    "\n",
    "\n",
    "\n",
    "for test_asset, model in spanning_results.items():\n",
    "    # print(model.params)\n",
    "    # print(model.pvalues)\n",
    "    print(f\"Results for CSM index country: {test_asset}\")\n",
    "    print(model.summary())\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35987897",
   "metadata": {},
   "source": [
    "The regression below presents the entire model. This is a multivariate model, meaning the lhs of the equation consists of all the test assets (N x t). In order to facilitate the regression tooling software, the data were transformed so that the dependent variable was one long vector of all the columns {r1, ..., rN} of the original matrix R.\n",
    "\n",
    "R -> [r1, r2, r3]'\n",
    "\n",
    "Its dimensions are now (N x t) x 1\n",
    "\n",
    "On the righthand side, the benchmark asset returns were 'repeated' so as to properly match the dependent variable data and new dimensions. The block of R (N x K) was extended N times. So that the dimensions are now (t x N) x K. Previously the shape was N x K.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6822106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined model:\n",
    "y_combined = csm_returns.values.flatten()\n",
    "X_combined = np.tile(\n",
    "    trad_country_returns.values, \n",
    "    (csm_returns.shape[1], 1)\n",
    ")\n",
    "X_combined = sm.add_constant(X_combined)\n",
    "\n",
    "combined_model = sm.OLS(y_combined, X_combined).fit()\n",
    "print(combined_model.summary())\n",
    "\n",
    "# f_test_result = combined_model.f_test(\"const = 0\")\n",
    "# print(f_test_result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a315f39a",
   "metadata": {},
   "source": [
    "The model shows that the regression coefficient of the intercept is different from zero with statistical significance (p < 0.001). The result that not all coefficients are (near) zero suggests that the benchmark assets cannot span the total return variance of test assets and therefore, that there are diversification benefits of the CSM indices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2dbca",
   "metadata": {},
   "source": [
    "# Efficient frontier\n",
    "We can also visualize the efficient frontiers for portfolios constructed using either country index. We construct the efficient frontier as follows. We find a set of weights w such that for various levels of returns, we minimize the volatility of the portfolio:\n",
    "\n",
    "min (w'Sw)^(1/2)\n",
    "\n",
    "s.t. sum(w) = 1\n",
    "and 0 <= w_i <= 1\n",
    "\n",
    "where w is an 1 x N vector of weights, for N number of countries and S is the N x N sample covariance matrix. The covariance matrix is constructed by simply taking the entire range of monthly returns for each asset. The returns are therefore in monthly terms, which is a bit odd to interpret but does not change the essence of the frontiers.\n",
    "\n",
    "We perform this exercise twice, once for the CSM indices and again for the traditional country indices. We can then compare the shape of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e79a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_efficient_frontier\n",
    "\n",
    "csm_mean_returns = csm_returns.mean().values\n",
    "csm_cov_matrix = csm_returns.cov()\n",
    "\n",
    "trad_mean_returns = trad_country_returns.mean().values\n",
    "trad_cov_matrix = trad_country_returns.cov()\n",
    "\n",
    "# combined_returns = pd.concat([csm_returns, trad_country_returns], axis=1)\n",
    "# combined_mean_returns = combined_returns.mean().values\n",
    "# combined_cov_matrix = combined_returns.cov()\n",
    "\n",
    "ef_csm = create_efficient_frontier(csm_mean_returns, csm_cov_matrix)\n",
    "ef_trad = create_efficient_frontier(trad_mean_returns, trad_cov_matrix)\n",
    "# ef_combined = create_efficient_frontier(combined_mean_returns, combined_cov_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02929a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(ef_csm.volatility * 100, ef_csm.returns * 100, label='Efficient Frontier csm', color='blue', linestyle='-')\n",
    "plt.plot(ef_trad.volatility * 100, ef_trad.returns * 100, label='Efficient Frontier trad', color='red', linestyle='-')\n",
    "# plt.plot(ef_combined.volatility * 100, ef_combined.returns * 100, label='Efficient Frontier combined', color='green', linestyle='--')\n",
    "plt.xlabel('Portfolio Volatility (Std. Dev.)')\n",
    "plt.ylabel('Portfolio Return')\n",
    "plt.title('Efficient Frontier')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbe6f0",
   "metadata": {},
   "source": [
    "# Monte Carlo simulations\n",
    "This section explores the diversification properties of the CSM indices versus their traditional country proponents using Monte Carlo simulations. For both index types (csm and trad) we randomly sample countries to construct portfolios. For each simulation run, we vary the number of countries we draw. So we randomly draw 2 countries, 3 countries, 5 countries, etc. Each draw, we calculate the equal-weighted portfolio returns of the sampled countries and calculate annualized (geometric) return, standard deviation of the returns and risk/reward ratio (the ratio of the first two metrics - called the Sharpe ratio in the graph, although it is technically not so). This is repeated 10.000 times. We repeat this procedure for specific numbers of countries (2, 3, 5, 7, 10, 13, 16) and for both index types. We finally proceed to aggregate the results by retaining only specific points from the obtained distribution. For each set of outcomes of 10.000 runs we select only the 2.5 percentile, 97.5 percentile and the median observations. \n",
    "\n",
    "The results are shown in the graphs below. The first panel shows only the median results for both index types, in one graph. The second panel also presents the lower (2.5 percentile) and upper (97.5 percentile) ranges of the distribution of the results. They are shown in separate plots for clarity. The third panel presents the results for the risk/reward ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98dc843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulations\n",
    "\n",
    "# load data\n",
    "results_quantiles = pd.read_csv('data/simulations_results_quantiles_n_runs_10000.csv', index_col=0)\n",
    "simulations.plot_std_dev(results_quantiles, metric=\"std_dev\", n_runs=10000, list_of_quantiles=[0.5], facet_plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b30d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulations\n",
    "\n",
    "# load data\n",
    "results_quantiles = pd.read_csv('data/simulations_results_quantiles_n_runs_10000.csv', index_col=0)\n",
    "simulations.plot_std_dev(results_quantiles, metric=\"std_dev\", n_runs=10000, list_of_quantiles=None, facet_plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0e6359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import simulations\n",
    "\n",
    "# load data\n",
    "results_quantiles = pd.read_csv('data/simulations_results_quantiles_n_runs_10000.csv', index_col=0)\n",
    "simulations.plot_std_dev(results_quantiles, metric=\"sharpe_ratio\", n_runs=10000, list_of_quantiles=None, facet_plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b955386",
   "metadata": {},
   "source": [
    "We can visually interpret these results as follows. Let's first look at the change in standard deviation as the number of countries in portfolio increases. We are not directly interested in absolute levels (these are slightly higher on average for CSM indices), we want to look at the shape of each graph and compare that. So this graph, for the CSM indices, starts at around 6.8% (for 2 countries) and ends at 5.9%. This is a decrease of 14%. For the trad indices, the curve starts at 6% (for 2 countries) and ends at 5.4%. This is a decrease of 10%. Moreover, the curve for the traditional country indices seems to be slightly more concave, meaning it approaches the asymptote faster. \n",
    "\n",
    "These results suggests that the diversification benefits of traditional country indices are depleted faster. It takes fewer different countries in the index to obtain a decrease in portfolio risk similar to holding all countries. A flipside of this observation is that one could say that these traditional country indices are more alike. For the CSM indices, it takes more countries to approximate the portfolio standard deviation of the full set of countries. This suggests that CSM indices are more different from one another than traditional country indices are.\n",
    "\n",
    "The statistics mentioned above in the text are summarized in the table below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a828f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_quantiles = pd.read_csv('data/simulations_results_quantiles_n_runs_10000.csv', index_col=0)\n",
    "csm_median_stddev_start = results_quantiles[(results_quantiles['dataset'] == 'csm') & (results_quantiles.index == 0.5) & (results_quantiles['num_countries'] == 2)]['std_dev']\n",
    "csm_median_stddev_end = results_quantiles[(results_quantiles['dataset'] == 'csm') & (results_quantiles.index == 0.5) & (results_quantiles['num_countries'] == 16)]['std_dev']\n",
    "\n",
    "trad_median_stddev_start = results_quantiles[(results_quantiles['dataset'] == 'trad') & (results_quantiles.index == 0.5) & (results_quantiles['num_countries'] == 2)]['std_dev']\n",
    "trad_median_stddev_end = results_quantiles[(results_quantiles['dataset'] == 'trad') & (results_quantiles.index == 0.5) & (results_quantiles['num_countries'] == 16)]['std_dev']\n",
    "\n",
    "pd.DataFrame({\n",
    "    'CSM': \n",
    "    {\n",
    "        'median_stddev_start (N=2)': csm_median_stddev_start.values[0],\n",
    "        'median_stddev_end (N=16)': csm_median_stddev_end.values[0],\n",
    "        'relative_change': csm_median_stddev_end.values[0] / csm_median_stddev_start.values[0] - 1\n",
    "    },\n",
    "    'Traditional': \n",
    "    {\n",
    "        'median_stddev_start (N=2)': trad_median_stddev_start.values[0],\n",
    "        'median_stddev_end (N=16)': trad_median_stddev_end.values[0],\n",
    "        'relative_change': trad_median_stddev_end.values[0] / trad_median_stddev_start.values[0] - 1\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f739cc4",
   "metadata": {},
   "source": [
    "# Section: return decomposition\n",
    "\n",
    "In this section we explore more closely the return characteristics of the CSM indices. Given that a CSM index is, by design, more closely linked to its sales footprint to a given country (and therefore by its cashflows), we formulate the expectation that the returns are more closely linked to news about cashflows than discount rates - or at least to cashflow news relating to the target country. This point is not trivial and we will develop that in more detail later. In order to test this hypothesis, we will perform a return decomposition of both the CSM indices and traditional country indices, so that we may compare the results. \n",
    "\n",
    "An established procedure to study this phenomenon, is through the present value decomposition of excess returns (Shiller and Campbell, 1988), through a vector autoregressive model (Campbell, 1991). This approach has been applied, in many variations, to several research questions in the literature (/cite).\n",
    "\n",
    "The methodology has its origins in present value models that show that the present value of an asset will be determined by two main components: (1) it will change with unexpected information/news about the cashflows from the asset. If expected cashflows go down, naturally, the value of the asset will go down as well, and vice versa; (2) it will change with the discount rate. If the discount rate with which the cashflows are valued goes down, future cashflows become more valuable. Therefore, the value of the asset will rise. An important distinction between the two components is that a change in discount rates will have an effect on the asset's value now, but also on the expected future return: as the discount rate goes down, initially the value of the asset will rise, but the future expected return will move in the other direction. For news about cashflows, this phenomenon does not occur. Positive (negative) cashflow news will simply lead to a one-off increase (decrease) in the asset's price.\n",
    "\n",
    "An important thing to note here is that we are talking about _expectations_. The rationale behind focusing on news (sometimes also called innovations) is that only new information will move prices. The (reasonable) assumption is that everything that is already known should be reflected in today's market prices.\n",
    "\n",
    "Campbell and Shiller's (1988a) original approach to formalize this notion was into the following seminal equation:\n",
    "\n",
    "r_{t+1} - E_t r_{t+1} = ...\n",
    "\n",
    "r_{t+1} - E_t r_{t+1} = N_{CF, t+1} - N_{DR, t+1}\n",
    "\n",
    "where N_CF denotes news about future cash flows, N_DR denotes news about future discount rates.\n",
    "\n",
    "This equation can be operationalized using Campbell's (1991) VAR approach. The VAR model is designed such that it first estimates the terms Et r_{T+1} (today's expectation of next period returns) and (E_{t+1} - E_t)\\sum(rho x r_{t+i+j}) (news about future discount rates). This gives us two of the three terms of equation 2; and we can calculate the remaining term N_{CF}. \n",
    "\n",
    "As in Campbell and Polk (2002), we assume the data generating process comes from the following VAR model:\n",
    "\n",
    "z_{t+1} = a + X z_{t} + u_{t+1}\n",
    "\n",
    "where z_{t+1} is an m x 1 state vector, a and X are constant parameters with dimensions of m x 1 respectively m x m; and u_{t+1} an i.i.d. m x 1 vector of shocks. The crux of the VAR model lies in the u_{t+1} vector. They are based on the residuals of the model and account for all variance of the asset's return that is not explained by the system of (lagged) state variables. This is important to note, as it underlines the importance of having the state variables correct. One characteristic of these types of VAR models is that the convention is that the first equation is always the lagged (one-period) return. This means the first element of z_{t+1} is r_{t+1}.\n",
    "\n",
    "In order to get from aggregate shocks to news on specifically cashflows and/or discount rates, we use the following mapping functions:\n",
    "\n",
    "N_{CF, t+1) = ... \n",
    "N_{DR, t+1) = ...\n",
    "\n",
    "Where e1 is a vector of length m with 1 as its first element and 0's otherwise. This effectively selects only the first row (return component) to measure news sensitivity. \\lambda is defined as \\lambda = \\rho X(I - \\rho X)^-1. \n",
    "\n",
    "# State variables\n",
    "There has been quite some variation in the literature on which state variables have been used (see Driessen en Van Zundert (2017) for an overview). As a starting point, we will use those that are commonly used in the literature. But we have to be cautious with using certain holdings / fundamentally based equity criteria. A state variable that is often used is the price/earnings ratio (of which there exist many variations). These are measured on an index level. For the US, the Shiller PE, which is publically available from Prof. Shiller's website, is commonly used. This US Shiller PE is based on price, dividends, earnings and inflation of the S&P500 index. However, for our research question, that may not be a suitable approach, since it is a very close manifestation of the traditional country index we are comparing the CSM index for the US against. Therefore, we will rely mostly on macro-economic factors of a country.\n",
    "\n",
    "There still is some value in performing the analysis using P/E ratio's, but we will only do this as a robustness check (and we have to be cautious interpreting the results). We will use two variations. One using the US CAPE ratio. Another using a global price/earnings ratio, derived from a global market index. The downsides of the first were already mentioned. The latter approach, using global P/E's, also comes with a downside, which is that it contains information that is not directly relevant to the specific country. If either traditional country indices or CSM indices are a substantially better proxy for a target country's risk, using global fundamentals may not be the best way to measure that. A benefit of using global fundamentals is that it can easily be applied to any country index.\n",
    "\n",
    "\n",
    "# Data\n",
    "In this initial version, we will focus on just the US. For the US, it is relatively easy to obtain appropriate data. Moreover, most of the literature on return decomposition is based on US data, which allows us to easier compare our findings with those in the literature.\n",
    "\n",
    "Suitable candidates for inclusion in the vector of state variables are:\n",
    "* Excess stock return - log returns. The period returns where convert to log returns. Then subtracted by the log returns of the 1M t-bill rate. Since we have monthly return data, that is the most appropriate value to calculate excess returns. Downloaded from FRED (US open source data). Actually the 4-week t-bill rate, comes closest to the monthly rate but that was not available.\n",
    "* 3M t-bill yield - converted to percentages. Downloaded from FRED (US open source data).\n",
    "* Yield spread (10Y-3M) - converted to percentages. Downloaded from FRED (US open source data).\n",
    "* Default spread - converted to percentages. Downloaded from FRED (US open source data). Default spread is the difference between the yield on a BAA corporate bond and a 10 year government bond.\n",
    "\n",
    "Robustness:\n",
    "* PE (US and global)\n",
    "    * for the US, the Shiller PE was used from Prof. Shiller's website. It was converted to log values.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ddc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate risk-free returns\n",
    "US_trad = trad_country_returns['US']\n",
    "US_trad = US_trad.reset_index()\n",
    "US_trad['Date'] = pd.to_datetime(US_trad['Date'])\n",
    "US_trad['US'] = np.log(1 + US_trad['US'])\n",
    "US_trad = US_trad.set_index('Date')\n",
    "US_trad = US_trad.join(df_market_data['DTB4WK'])\n",
    "US_trad_excess = US_trad['US'] - US_trad['DTB4WK']\n",
    "\n",
    "\n",
    "US_csm = csm_returns['US']\n",
    "US_csm = US_csm.reset_index()\n",
    "US_csm['Date'] = pd.to_datetime(US_csm['Date'])\n",
    "US_csm['US'] = np.log(1 + US_csm['US'])\n",
    "US_csm = US_csm.set_index('Date')\n",
    "US_csm = US_csm.join(df_market_data['DTB4WK'])\n",
    "US_csm_excess = US_csm['US'] - US_csm['DTB4WK']\n",
    "\n",
    "\n",
    "df_market_data = df_market_data.join(US_trad_excess.rename('US_trad_excess'), how='left')\n",
    "df_market_data = df_market_data.join(US_csm_excess.rename('US_csm_excess'), how='left')\n",
    "df_market_data = df_market_data.dropna()\n",
    "\n",
    "# remove duplicates after joining\n",
    "df_market_data = df_market_data[~df_market_data.index.duplicated(keep='first')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07519497",
   "metadata": {},
   "outputs": [],
   "source": [
    "import var_decomposition\n",
    "\n",
    "data_VAR_trad = df_market_data[[\n",
    "    \"US_trad_excess\",\n",
    "    \"BAA10Y\",\n",
    "    \"T10Y3M\",\n",
    "    \"DTB3\",\n",
    "    # \"CAPE\"\n",
    "]]\n",
    "\n",
    "N_CF, N_DR, results = var_decomposition.get_var_decomp(data_VAR_trad)\n",
    "\n",
    "corr_mat = np.corrcoef(N_CF, N_DR)\n",
    "std_N_CF = np.std(N_CF)\n",
    "std_N_DR = np.std(N_DR)\n",
    "np.fill_diagonal(corr_mat, [std_N_CF, std_N_DR])\n",
    "pd.DataFrame(corr_mat, index=['CF', 'DR'], columns=['CF', 'DR']).round(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ea1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import var_decomposition\n",
    "\n",
    "data_VAR_csm = df_market_data[[\n",
    "    \"US_csm_excess\",\n",
    "    \"BAA10Y\",\n",
    "    \"T10Y3M\",\n",
    "    \"DTB3\",\n",
    "    # \"CAPE\"\n",
    "]]\n",
    "\n",
    "\n",
    "N_CF, N_DR, results = var_decomposition.get_var_decomp(data_VAR_csm)\n",
    "\n",
    "corr_mat = np.corrcoef(N_CF, N_DR)\n",
    "std_N_CF = np.std(N_CF)\n",
    "std_N_DR = np.std(N_DR)\n",
    "np.fill_diagonal(corr_mat, [std_N_CF, std_N_DR])\n",
    "pd.DataFrame(corr_mat, index=['CF', 'DR'], columns=['CF', 'DR']).round(4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c96356",
   "metadata": {},
   "source": [
    "The two tables above show the correlation between the cashflow and discount rate news components (off-diagonial) and standard deviations (diagonal). The first table's results are for the traditional US country index; the second table is for the CSM US index. Unlike what we initially hypothesized, the results for the two index types are quite similar. Let's first look at the proportion of the standard deviation that each variable explains. These figures (and their proportion) represent how dominant each news type is in explaining a country's market returns. For the traditional country index, the standard deviations of cashflows news (4.8%) and discount rate news (5%) are almost the same. For the CSM indices, it is 5.3% for cashflow news and 6% for discount rate news. These results suggest that the proportion is quite similar; even though we had hypothesized that the cashflow news component would have been more dominant for the CSM indices. If anything, discount rates news is slightly more important for CSM indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3676a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "N_CF_trad, N_DR_trad, results_trad = var_decomposition.get_var_decomp(data_VAR_trad)\n",
    "N_CF_csm, N_DR_csm, results_csm = var_decomposition.get_var_decomp(data_VAR_csm)\n",
    "\n",
    "df_for_plot = pd.DataFrame({\n",
    "    'Date': data_VAR_trad.reset_index()['Date'][:-1],\n",
    "    'N_CF_trad': N_CF_trad,\n",
    "    'N_DR_trad': N_DR_trad,\n",
    "    'N_CF_csm': N_CF_csm,\n",
    "    'N_DR_csm': N_DR_csm,\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(16, 6), sharey=True)\n",
    "\n",
    "facet_info = [\n",
    "    ('N_CF_trad', 'N_DR_trad', 'Traditional Index'),\n",
    "    ('N_CF_csm', 'N_DR_csm', 'CSM Index')\n",
    "]\n",
    "\n",
    "for ax, (cf_col, dr_col, title) in zip(axes, facet_info):\n",
    "    ax.plot(df_for_plot['Date'], df_for_plot[cf_col], label='a. Cashflow News', color='tab:blue', linestyle='-')\n",
    "    ax.plot(df_for_plot['Date'], df_for_plot[dr_col], label='b. Discount Rate News', color='tab:orange', linestyle='--')\n",
    "    ax.set_title(title)\n",
    "    # if ax is axes[0]:\n",
    "    #     ax.set_xlabel('Date')\n",
    "    ax.grid(True)\n",
    "    ax.set_ylabel('News Component')\n",
    "    ax.legend()\n",
    "\n",
    "plt.suptitle('Cashflow and Discount Rate News: Trad and CSM')\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e68f685",
   "metadata": {},
   "source": [
    "The figure above plots the timeseries of the shocks mapped to each news type; the results are shown distinctly for the traditional US index and the CSM US index. Visually, the charts look very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c9eab6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354105f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
